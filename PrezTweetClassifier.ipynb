{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "useNLTK = True\n",
    "\n",
    "import csv\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "if useNLTK:\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hc=pd.read_csv(\"HillaryClinton_tweets.csv\")\n",
    "jb=pd.read_csv(\"JebBush_tweets.csv\")\n",
    "tc=pd.read_csv(\"TedCruz_tweets.csv\")\n",
    "jc=pd.read_csv(\"DrJillStein_tweets.csv\")\n",
    "dt=pd.read_csv(\"realDonaldTrump_tweets.csv\")\n",
    "bs=pd.read_csv(\"BernieSanders_tweets.csv\")\n",
    "\n",
    "candidates = [hc,dt,jb,tc,jc,bs]\n",
    "candidatesText = [x[\"text\"] for x in candidates]\n",
    "classes = [\"Hillary Clinton\",\"Donald Trump\",\"Jeb Bush\",\"Ted Cruz\",\"Jill Stein\",\"Bernie Sanders\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(txt):\n",
    "    # see https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(r'[^\\w\\s]','',txt)\n",
    "    txt = re.sub(r\"http\\S+\", \"\", txt)\n",
    "    txt = re.sub(r\"https\\S+\", \"\", txt)\n",
    "    if useNLTK:\n",
    "        lemmatizedList = []\n",
    "        WNL = WordNetLemmatizer()\n",
    "        for word in re.split(\"\\W+\", txt):\n",
    "            lemmatizedList.append(WNL.lemmatize(word))\n",
    "        txt = ' '.join(lemmatizedList)\n",
    "    return txt\n",
    "\n",
    "def tokenize(txt):\n",
    "    return re.split(\"\\W+\", txt)\n",
    "\n",
    "def countWords(txt):\n",
    "    count = Counter(tokenize(txt))\n",
    "    count = dict(count)\n",
    "    try:\n",
    "        del count[\"\"]\n",
    "    except KeyError:\n",
    "        pass\n",
    "    return dict([a, float(x)] for a, x in count.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:02,  2.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sklearn implementation\n",
    "\n",
    "import numpy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def buildDataFrame():\n",
    "    d = []\n",
    "    i = []\n",
    "    for ind, candidate in tqdm(enumerate(candidates)):\n",
    "        for rowInd, row in candidate.iterrows():\n",
    "            d.append(row[\"text\"])\n",
    "        i.extend([classes[ind] for _ in candidate.itertuples()])\n",
    "    return pd.DataFrame({\"text\":d, \"class\":i})\n",
    "\n",
    "skdf = buildDataFrame()\n",
    "countVectorizer = CountVectorizer()\n",
    "counts = countVectorizer.fit_transform(skdf[\"text\"].values)\n",
    "classifier = MultinomialNB()\n",
    "targets = skdf['class'].values\n",
    "classifier.fit(counts, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [01:18, 13.12s/it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 407.00it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 21.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train(method):\n",
    "    PC = 1/len(classes)\n",
    "    PXC = dict.fromkeys(classes)\n",
    "    totalWords = dict.fromkeys(classes)\n",
    "    if method == 0:\n",
    "        totalDocs = dict.fromkeys(classes)\n",
    "        for doc in skdf.itertuples():\n",
    "            if PXC[doc[1]] == None:\n",
    "                PXC[doc[1]] = dict()\n",
    "            if totalDocs[doc[1]] == None:\n",
    "                totalDocs[doc[1]] = 0\n",
    "            for word in tokenize(doc[2]):\n",
    "                try:\n",
    "                    PXC[doc[1]][word] += 1\n",
    "                except KeyError:\n",
    "                    PXC[doc[1]][word] = 1\n",
    "            totalDocs[doc[1]] += 1\n",
    "        for candidate, words in PXC.items():\n",
    "            for word, instances in words.items():\n",
    "                PXC[candidate][word] = instances/totalDocs[candidate]\n",
    "    elif method == 1:\n",
    "        for ind, candidate in tqdm(enumerate(classes)):\n",
    "            for tweet in candidatesText[ind]:\n",
    "                tweet = clean(tweet)\n",
    "                if PXC[candidate]==None:\n",
    "                    PXC[candidate]=[]\n",
    "                PXC[candidate].append(Counter(countWords(tweet)))\n",
    "                if totalWords[candidate]==None:\n",
    "                    totalWords[candidate]=0\n",
    "                totalWords[candidate]+=len(tokenize(tweet))  \n",
    "            PXC[candidate] = dict(sum(PXC[candidate],Counter()))\n",
    "        for candidate in tqdm(classes):\n",
    "            for word,i in PXC[candidate].items():\n",
    "                PXC[candidate][word] = i/totalWords[candidate]\n",
    "    else:\n",
    "        raise TypeError\n",
    "    \n",
    "    for candidate in tqdm(classes):\n",
    "        allWords = sum([list(PXC[x].keys()) for x in classes], [])\n",
    "        allWordsLength = len(allWords)\n",
    "        PX = list(dict(Counter(allWords)).items())\n",
    "        PX = {i[0]:i[1]/allWordsLength for i in PX}\n",
    "    return PC, PXC, PX\n",
    "\n",
    "    \n",
    "\n",
    "PC, PXC, PX = train(method=1)\n",
    "print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import e, log as ln\n",
    "\n",
    "def predictClass(inp):\n",
    "    inp = clean(inp)\n",
    "    probabilities = dict.fromkeys(classes)\n",
    "    for candidate in classes:\n",
    "        if probabilities[candidate]==None:\n",
    "            probabilities[candidate] = 0\n",
    "        for word in tokenize(inp):\n",
    "            try:\n",
    "                probability = (PC*PXC[candidate][word])/PX[word]\n",
    "            except KeyError:\n",
    "                probability = 0\n",
    "            try:\n",
    "                probabilities[candidate] += ln(probability)\n",
    "            except ValueError:\n",
    "                pass\n",
    "    for candidate, probability in probabilities.items():\n",
    "        probabilities[candidate] = e**probability\n",
    "    mostLikely = sorted(list(probabilities.items()), key=lambda x: x[1])[-1]\n",
    "    if mostLikely[1]==0:\n",
    "        return False\n",
    "    else:\n",
    "        #pprint(probabilities)\n",
    "        return mostLikely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:00, 20.45it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5072886297376094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:07,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46282614311547404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def validateModel(useTrainingSet=False):\n",
    "    if useTrainingSet:\n",
    "        validationSet = [pd.read_csv(\"HillaryClinton_tweets.csv\"), \\\n",
    "            pd.read_csv(\"realDonaldTrump_tweets.csv\"), pd.read_csv(\"JebBush_tweets.csv\"), \\\n",
    "            pd.read_csv(\"TedCruz_tweets.csv\"), pd.read_csv(\"DrJillStein_tweets.csv\"), \\\n",
    "            pd.read_csv(\"BernieSanders_tweets.csv\")]\n",
    "    else:\n",
    "        validationSet = [pd.read_csv(\"HillaryClinton_tweets NEW.csv\"), \\\n",
    "            pd.read_csv(\"realDonaldTrump_tweets NEW.csv\"), pd.read_csv(\"JebBush_tweets NEW.csv\"), \\\n",
    "            pd.read_csv(\"TedCruz_tweets NEW.csv\"), pd.read_csv(\"DrJillStein_tweets NEW.csv\"), \\\n",
    "            pd.read_csv(\"BernieSanders_tweets NEW.csv\")]\n",
    "    \n",
    "    validationSet = [x[\"text\"] for x in validationSet]\n",
    "    totalTweets = 0\n",
    "    correctTweets = 0\n",
    "    for ind, candidate in tqdm(enumerate(classes)):\n",
    "        for validate in validationSet[ind]:\n",
    "            totalTweets += 1\n",
    "            try:\n",
    "                if predictClass(validate)[0] == candidate:\n",
    "                    correctTweets += 1\n",
    "            except TypeError:\n",
    "                pass\n",
    "    return correctTweets/totalTweets\n",
    "\n",
    "print(validateModel())\n",
    "print(validateModel(useTrainingSet=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bernie Sanders', 17.37172624455413)\n",
      "('Donald Trump', 3.816016115301887)\n",
      "('Bernie Sanders', 2.713632339923281)\n",
      "('Jeb Bush', 1009.3878428802311)\n",
      "('Donald Trump', 356.19054136576557)\n"
     ]
    }
   ],
   "source": [
    "print(predictClass(\"Memes are responsible for climate change\"))\n",
    "print(predictClass(\"I love hispanics\"))\n",
    "print(predictClass(\"Billionaires suck\"))\n",
    "print(predictClass(\"Let us become a glorious socialist empire and serve the motherland\"))\n",
    "print(predictClass(\"Let's make America Great again!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hillary Clinton'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = [\"Let us become a glorious socialist empire and serve the motherland\"]\n",
    "example_counts = countVectorizer.transform(examples)\n",
    "predictions = classifier.predict(example_counts)\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:00,  7.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.749271137026239"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validateModelSKLearn(useTrainingSet=False):\n",
    "    if useTrainingSet:\n",
    "        validationSet = [pd.read_csv(\"HillaryClinton_tweets.csv\"), \\\n",
    "            pd.read_csv(\"realDonaldTrump_tweets.csv\"), pd.read_csv(\"JebBush_tweets.csv\"), \\\n",
    "            pd.read_csv(\"TedCruz_tweets.csv\"), pd.read_csv(\"DrJillStein_tweets.csv\"), \\\n",
    "            pd.read_csv(\"BernieSanders_tweets.csv\")]\n",
    "    else:\n",
    "        validationSet = [pd.read_csv(\"HillaryClinton_tweets NEW.csv\"), \\\n",
    "            pd.read_csv(\"realDonaldTrump_tweets NEW.csv\"), pd.read_csv(\"JebBush_tweets NEW.csv\"), \\\n",
    "            pd.read_csv(\"TedCruz_tweets NEW.csv\"), pd.read_csv(\"DrJillStein_tweets NEW.csv\"), \\\n",
    "            pd.read_csv(\"BernieSanders_tweets NEW.csv\")]\n",
    "    \n",
    "    validationSet = [x[\"text\"] for x in validationSet]\n",
    "    totalTweets = 0\n",
    "    correctTweets = 0\n",
    "    for ind, candidate in tqdm(enumerate(classes)):\n",
    "        for validate in validationSet[ind]:\n",
    "            totalTweets += 1\n",
    "            samples = [validate]\n",
    "            sampleCounts = countVectorizer.transform(samples)\n",
    "            predictions = classifier.predict(sampleCounts)\n",
    "            prediction = predictions[0]\n",
    "            try:\n",
    "                if prediction == candidate:\n",
    "                    correctTweets += 1\n",
    "            except TypeError:\n",
    "                pass\n",
    "    return correctTweets/totalTweets\n",
    "\n",
    "validateModelSKLearn(useTrainingSet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in skdf:\n",
    "    print(i.loc([\"class\"]).value)\n",
    "    raise IndexError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
